
## 2.2 Logistic Regression 逻辑回归

`sigmoid`函数：$\sigma = \frac 1 {1+ e^{-z}}$，值域$(0, 1)$

输入：$\{ (x^{(1)}, y^{(1)}), ..., (x^{(m)}, y^{(m)}) \}$
输出函数：$\hat y = \sigma (w^T x + b)$
损失函数：$L(\hat y, y) = -y \log(\hat y) - (1-y) \log (1-\hat y)$
代价函数：$J(w, b) = \frac 1 m \sum L(\hat y^{(i)}, y^{(i)})$

## 3.6 Activation functions 激活函数

双曲正切函数  $\tanh(x) = \frac {e^x- e^{-x}} {e^x+ e^{-x}}$，值域$(-1, 1)$
> 有一点要说明：我基本已经不用`sigmoid`激活函数了，`tanh`在所有场合都优于`sigmoid`函数。
> 但有一个例外：在二分类问题中，对于输出层，因为$y$值是$0$或$1$，所以需要使用`sigmoid`激活函数，即对隐藏层使用`tanh`，输出层使用`sigmoid`函数。


修正线性单元ReLu：$a = \max(0, x)$，Leaky ReLu：$a = \max(0.01x, x)$
> 如果再隐藏层上不确定使用哪个激活函数，那么通常会使用ReLu激活函数。

## 3.11 随机初始化

权重不能全初始化为0：隐藏单元始终计算相同的函数。

$W^{[i]} = randn(2, 2)*0.01 \quad b^{[i]} = zeros(2, 1)$
乘以0.01：获得较小的初始值，避免激活函数的输入过大，比如`sigmoid`或`tanh`，在输入较大时导数很小（梯度弥散），训练速度慢；

---

## 2.6 Gradient descent with Momentum 动量梯度下降法 

指数加权平均：每个新的平均数由上一个平均数和当前值做插值，$v_{i+1} = v_i + 0.1 \theta, \quad v_0 = 0$
很显然的在初期这个估计值会偏小（偏向于$v_0$），因此可以进行倍率修正，即 $v_i = \frac {v_i} {1- \beta ^i}$

使用指数加权平均来计算下降方向，而非直接使用负梯度：
$$ v_{dW} = \beta v_{dW} + (1-\beta) dW $$
$$ v_{db} = \beta v_{db} + (1-\beta) db $$
$$ W = W- \alpha v_{dW}, \quad b= b- \alpha v_{db} $$
直观来说，与历史方向进行平均一定程度削弱了下降中的震荡。

类似地，RMSprop算法（root mean square prop）
$$ S_{dW} = \beta S_{dW} + (1-\beta) （dW)^2,\quad S_{db} = \beta S_{db}+ (1-\beta) (db)^2 $$
$$ W = W- \alpha \frac {dW} {\sqrt {S_{dW}}}, \quad b = b -\alpha \frac {db} {\sqrt {S_{db}}} $$
用模来“归一化”了各方向的梯度。

而Adam算法（Adaptive Moment Estimation）似乎就只是结合了这两者，可以收敛更快。

## 3.4 Normalizing activations in a network 归一化网络激活值

Batch归一化，简单来说就是归一化隐藏层的输入，即在隐藏层间做类似输入的归一化，以提高下一层的收敛速度。

另外，作者推荐的是在激活函数之前归一化，即归一化$z^{[i]}$而非$a^{[i]}$，同时，并不一定说分布在$(0, 1)$就比较好，所以还会做一个变换：$\tilde z ^ {[i]} = \gamma z_{norm}^{[i]} + \beta$，如此，$\gamma$、$\beta$也成为需要学习的参数。

## 3.8 Softmax regression

类似作为输出层的激活函数，但接受向量进行指数归一化，描述各类的概率。
```python3
t = np.exp(z_l)
y = t / np.sum(t)
```

其名称与hardmax对应，即不是直接选择输出层$z^{[l]}$这的最大值。

损失函数：$L(\hat y, y) = - \sum_j y_j \ln {\hat y_j}$，类似逻辑回归。

---

## 4.2 Deep convolutional models: case studies

**ResNets** 添加了跳跃连接（Skip connection）之后，可以用更深的模型。直观上它可以保留原来的性能：skip被“激活”时；

**Inception network motivation** 更多冗余的网络块，在学习中自动的取舍，从这一点上来，和ResNets有相似之处；

## 4.4 Special applications: Face recognition &Neural style transfer

Siamese network：使用一个网络来对不同图像进行压缩，之后在此基础上进行其他算法判定；

triplet损失：同时查看三个输入，$A, N, P$希望$$ d(A, P) + \alpha \le d(A, N) \quad (where\ \alpha > 0)$$
也就是说希望anchor和positive的距离要足够小于和negative的距离；



