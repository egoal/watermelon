### 2. Descent Method

$$ F(x+\alpha h) = F(x)+\alpha h^T F'(x) +O(\alpha^2) \\
	\simeq F(x)+\alpha h^T F'(x) $$

$h$ is a desent direction for $F$ at $x$ if $h^TF'(x)<0$.

##### 2.1. The Steepest Descent method

$$ h_{sd} = -F'(x) $$

##### 2.2. Newton's Method

note that stationary point $x^*$ satisfies $F'(x)=0$, and:

$$ F'(x+h) = F'(x)+hF''(x) +O(||h||^2) \\
	\simeq F'(x)+hF''(x) $$

so, compute the next iterate by:
$$ Hh_n = -F'(x) \\ H=F''(x) \\ x: = x+h_n $$

suppose that $H$ is positive definite, then $ 0 < h_n^THh_n = -h_n^TF'(x) $, $h_n$ is a descent direction.

##### 2.3. Line Search

study the variation of the given function along the half line from $x$ in the direction $h$:
$$ \varphi(\alpha) = F(x+\alpha h), a\geq 0 $$

this guide the step $\alpha$ selection:
* the gain in objective function is very small, increase $ \alpha $
* $\varphi(\alpha) \ge \varphi(0)$, decrease $\alpha$
* $\alpha$ is close to minimizer of $\varphi(\alpha)$, accept

note that though a accurate $\alpha$ can be calculated, no need to do this when $x$ is far from $x^*$.

##### 2.4. Trust Region and Damped Methods

$$ F(x+h) \simeq L(h) \equiv F(x)+h^Tc+ \frac12 h^TBh $$

we assume that $L(h)$ is sufficiently accurate inside a ball with radius $\Delta$, so:
$$ h_{tr} \equiv arg \min _{||h|| \le \Delta} L(h) $$

damped method:
$$ h_{dm} \equiv arg \min _h L(h)+\frac12 \mu h^Th $$
$\frac12 \mu h^Th$ penalize large steps

consider dumped version:
$$ \psi _{\mu}(h) = L(h) + \frac12 \mu h^Th $$
$$ \psi'_{\mu}(h) = L'(h)+\mu h = c+Bh+uh $$
we have:
$$ (B+\mu I)h_{dm} = -c $$

by applay $c=F'(x)$, $B=F''(x)$, got damped Newton method:
$$ (F''(x)+\mu I)h_{dn} = -F'(x) $$
a hybrid of between $h_{ds}$ and $h_n$.

***

### 3. Non-Linear Least Squares Problems

$$ x^* = arg \min_x \{F(x)\} $$
where:
$$ F(x) = \frac 12 \sum_{i=1}^m (f_i(x))^2 = \frac 12 f(x)^Tf(x) $$

we have:
$$ \frac {\partial F}{\partial x_j} (x) = \sum_{i=1}^m f_i(x)
\frac {\partial f_i}{\partial x_j} (x)$$
$$ \frac {\partial ^2F}{\partial x_j \partial x_k} = \sum_{i=1}^m (
\frac {\partial f_i}{\partial x_j} \frac {\partial f_i}{\partial x_k}
+f_i(x)\frac {\partial ^2 f_i}{\partial x_j \partial x_k})$$

thus:
$$ F'(x)=J(x)^Tf(x) $$
$$ F''(x) = J(x)^TJ(x) + \sum_{i=1}^m f_i(x) f_i''(x) $$

##### 3.1. The Gauss-Newton Method

let $ f(x+h) \simeq l(h) \equiv f(x)+J(x)h $:
$$ (J^TJ)h_{gn} = -J^Tf $$

##### 3.2. The Levenberg-Marquadt Method

$$ (J^TJ + \mu I)h_{lm} = -J^Tf $$

gain predicted by the linear model:
$$ L(0)-L(h_{lm}) = -h_{lm}^TJ^Tf - \frac 12 h_{lm}^TJ^TJh_{lm} \\
= \frac 12 h_{lm}^T (\mu h_{lm} -g) $$

##### 3.3. Powell's Dog Leg Method

##### 3.4. A Hybrid Method: L-M and Quasi-Newton

Quasi-Newton method is based on having B approximate to Hessian $F''(x)$, then:
$$ Bh_{qn}=-F' $$
$B$ is updated by BFGS strategy.
The Quasi-Newton method is not robust in the global stage: not guaranteed to be descenting.

if:
$$ ||F'(x)||_\infty < 0.02*F(x) $$
$F(x^*)$ is significantly nonzero, swith to Quasi-Newton for better performance.
A good final convergence is indicated by rapidly decreasing values of $||F'(x)||$, switch back to L-M if these norm values do noe decrease rapidly enough.

##### 3.5. A Secant Version of the L-M Method













